{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xmffZS_306eb"
   },
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "S9ywsUmT05C1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-12 05:57:57.481105: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "import sonnet as snt\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Xx--Nco09fN"
   },
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tfenformer #the output of this enformer has been changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "2BuZ2gmUbpXZ"
   },
   "outputs": [],
   "source": [
    "# @title `get_dataset(organism, subset, num_threads=8)`\n",
    "import glob\n",
    "import json\n",
    "import functools\n",
    "\n",
    "\n",
    "def organism_path(organism):\n",
    "    return os.path.join('/work/long_lab/qli/Enformer_TFs/', organism)\n",
    "\n",
    "\n",
    "def get_dataset(organism, subset, num_threads=16):\n",
    "    metadata = get_metadata(organism)\n",
    "    dataset = tf.data.TFRecordDataset(tfrecord_files(organism, subset),\n",
    "                                    compression_type='ZLIB',\n",
    "                                    num_parallel_reads=num_threads)\n",
    "    dataset = dataset.map(functools.partial(deserialize, metadata=metadata),\n",
    "                        num_parallel_calls=num_threads)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def get_metadata(organism):\n",
    "    # Keys:\n",
    "    # num_targets, train_seqs, valid_seqs, test_seqs, seq_length,\n",
    "    # pool_width, crop_bp, target_length\n",
    "    path = os.path.join(organism_path(organism), 'statistics.json')\n",
    "    with tf.io.gfile.GFile(path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def tfrecord_files(organism, subset):\n",
    "  # Sort the values by int(*).\n",
    "  return sorted(tf.io.gfile.glob(os.path.join(\n",
    "      organism_path(organism), 'tfrecords', f'{subset}-*.tfr'\n",
    "  )), key=lambda x: int(x.split('-')[-1].split('.')[0]))\n",
    "\n",
    "\n",
    "def deserialize(serialized_example, metadata):\n",
    "    \"\"\"Deserialize bytes stored in TFRecordFile.\"\"\"\n",
    "    feature_map = {\n",
    "      'sequence': tf.io.FixedLenFeature([], tf.string),\n",
    "      'target': tf.io.FixedLenFeature([], tf.string),\n",
    "    }\n",
    "    example = tf.io.parse_example(serialized_example, feature_map)\n",
    "    sequence = tf.io.decode_raw(example['sequence'], tf.bool)\n",
    "    sequence = tf.reshape(sequence, (metadata['seq_length'], 4))\n",
    "    sequence = tf.cast(sequence, tf.float32)\n",
    "\n",
    "    target = tf.io.decode_raw(example['target'], tf.float16)\n",
    "    target = tf.reshape(target,\n",
    "                      (metadata['target_length'], metadata['num_targets']))\n",
    "    target = tf.cast(target, tf.float32)\n",
    "\n",
    "    return {'sequence': sequence,\n",
    "          'target': target}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VzGRXfwV4tYH"
   },
   "source": [
    "## 1. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "YDSKttXI4hMT"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-12 05:58:08.885151: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-12-12 05:58:08.886839: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2021-12-12 05:58:10.468096: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:3b:00.0 name: Tesla V100-PCIE-16GB computeCapability: 7.0\n",
      "coreClock: 1.38GHz coreCount: 80 deviceMemorySize: 15.78GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2021-12-12 05:58:10.468744: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties: \n",
      "pciBusID: 0000:d8:00.0 name: Tesla V100-PCIE-16GB computeCapability: 7.0\n",
      "coreClock: 1.38GHz coreCount: 80 deviceMemorySize: 15.78GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2021-12-12 05:58:10.468770: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-12-12 05:58:10.578711: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2021-12-12 05:58:10.578800: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
      "2021-12-12 05:58:10.583430: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2021-12-12 05:58:10.598801: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2021-12-12 05:58:10.626871: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2021-12-12 05:58:10.630842: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
      "2021-12-12 05:58:10.631354: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /work/long_lab/qli/biosoft/lib\n",
      "2021-12-12 05:58:10.631366: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1757] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2021-12-12 05:58:10.631893: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-12-12 05:58:10.632752: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-12-12 05:58:10.632775: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-12-12 05:58:10.632780: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      \n"
     ]
    }
   ],
   "source": [
    "human_dataset = get_dataset('human', 'train').batch(1).repeat()\n",
    "#mouse_dataset = get_dataset('mouse', 'train').batch(1).repeat()\n",
    "human_dataset_pre = human_dataset.prefetch(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SHHNHzFVbvTk"
   },
   "source": [
    "## 2. Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "0U3hLJaUdZkG"
   },
   "outputs": [],
   "source": [
    "def create_step_function(model, optimizer):\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(batch, head, optimizer_clip_norm_global=0.2):\n",
    "        with tf.GradientTape() as tape:\n",
    "          outputs = model(batch['sequence'], is_training=True)[head]\n",
    "          loss = tf.reduce_mean(\n",
    "              tf.keras.losses.poisson(batch['target'], outputs))\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply(gradients, model.trainable_variables)\n",
    "\n",
    "    return loss\n",
    "  return train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ZXv5HU_242Ut"
   },
   "outputs": [],
   "source": [
    "learning_rate = tf.Variable(0., trainable=False, name='learning_rate')\n",
    "optimizer = snt.optimizers.Adam(learning_rate=learning_rate)\n",
    "num_warmup_steps = 0\n",
    "target_learning_rate = 0.0005\n",
    "\n",
    "model = tfenformer.Enformer(channels=1536,  # Use 4x fewer channels to train faster.\n",
    "                          num_heads=8,\n",
    "                          num_transformer_layers=11,\n",
    "                          pooling_type='attention')\n",
    "\n",
    "train_step = create_step_function(model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cellView": "code",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FrbDaOMWcFUl",
    "outputId": "6a42f69c-3003-47f2-a8d2-1b94c52eb57e"
   },
   "outputs": [],
   "source": [
    "### Train the model (Do only if you need to start from scratch)\n",
    "\n",
    "checkpoint_root = \"TFEnformer_Transfer_Train_all_trainable_variables/checkpoints\"\n",
    "checkpoint_name = \"example\"\n",
    "save_prefix = os.path.join(checkpoint_root, checkpoint_name)\n",
    "checkpoint = tf.train.Checkpoint(module=model)\n",
    "latest = tf.train.latest_checkpoint(checkpoint_root)\n",
    "if latest is not None:\n",
    "    checkpoint.restore(latest)\n",
    "\n",
    "steps_per_epoch = 20\n",
    "start_epoch = 0\n",
    "num_epochs = 200\n",
    "\n",
    "data_it = iter(human_dataset_pre)\n",
    "global_step = 0\n",
    "for epoch_i in range(start_epoch, num_epochs):\n",
    "    loss_log=open(\"TFEnformer_Transfer_Train_all_trainable_variables/loss_log_human\",\"a\")\n",
    "    for i in tqdm(range(steps_per_epoch)):\n",
    "        global_step += 1\n",
    "\n",
    "        if global_step > 1:\n",
    "            learning_rate_frac = tf.math.minimum(1.0, global_step / tf.math.maximum(1.0, num_warmup_steps))      \n",
    "            learning_rate.assign(target_learning_rate * learning_rate_frac)\n",
    "\n",
    "        batch_human = next(data_it)\n",
    "\n",
    "        loss_human = train_step(batch=batch_human, head='human')\n",
    "        #End of the step\n",
    "        print('global_step',global_step)\n",
    "\n",
    "    # End of epoch.\n",
    "    print('epoch_i',epoch_i,'loss_human', loss_human.numpy(),'learning_rate', optimizer.learning_rate.numpy())\n",
    "    now = datetime.now()\n",
    "    date_time = now.strftime(\"%m/%d/%Y, %H:%M:%S\")\n",
    "    loss_log.write(\"date and time: \"+ date_time+\", epoch: \"+str(epoch_i)+\", global_step: \"+str(global_step)+\", loss_human: \"+str(loss_human.numpy())+', learning_rate:'+str(optimizer.learning_rate.numpy())+', learning_rate: '+str(optimizer.learning_rate.numpy())+\"\\n\")\n",
    "    loss_log.close()\n",
    "    if epoch_i and not epoch_i % 2:\n",
    "        checkpoint.save(save_prefix)\n",
    "checkpoint.save(save_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title `PearsonR` and `R2` metrics\n",
    "\n",
    "def _reduced_shape(shape, axis):\n",
    "    if axis is None:\n",
    "        return tf.TensorShape([])\n",
    "    return tf.TensorShape([d for i, d in enumerate(shape) if i not in axis])\n",
    "\n",
    "\n",
    "class CorrelationStats(tf.keras.metrics.Metric):\n",
    "  \"\"\"Contains shared code for PearsonR and R2.\"\"\"\n",
    "\n",
    "  def __init__(self, reduce_axis=None, name='pearsonr'):\n",
    "    \"\"\"Pearson correlation coefficient.\n",
    "\n",
    "    Args:\n",
    "      reduce_axis: Specifies over which axis to compute the correlation (say\n",
    "        (0, 1). If not specified, it will compute the correlation across the\n",
    "        whole tensor.\n",
    "      name: Metric name.\n",
    "    \"\"\"\n",
    "    super(CorrelationStats, self).__init__(name=name)\n",
    "    self._reduce_axis = reduce_axis\n",
    "    self._shape = None  # Specified in _initialize.\n",
    "\n",
    "    def _initialize(self, input_shape):\n",
    "        # Remaining dimensions after reducing over self._reduce_axis.\n",
    "        self._shape = _reduced_shape(input_shape, self._reduce_axis)\n",
    "\n",
    "        weight_kwargs = dict(shape=self._shape, initializer='zeros')\n",
    "        self._count = self.add_weight(name='count', **weight_kwargs)\n",
    "        self._product_sum = self.add_weight(name='product_sum', **weight_kwargs)\n",
    "        self._true_sum = self.add_weight(name='true_sum', **weight_kwargs)\n",
    "        self._true_squared_sum = self.add_weight(name='true_squared_sum',\n",
    "                                                 **weight_kwargs)\n",
    "        self._pred_sum = self.add_weight(name='pred_sum', **weight_kwargs)\n",
    "        self._pred_squared_sum = self.add_weight(name='pred_squared_sum',\n",
    "                                                 **weight_kwargs)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        \"\"\"Update the metric state.\n",
    "\n",
    "        Args:\n",
    "          y_true: Multi-dimensional float tensor [batch, ...] containing the ground\n",
    "            truth values.\n",
    "          y_pred: float tensor with the same shape as y_true containing predicted\n",
    "            values.\n",
    "          sample_weight: 1D tensor aligned with y_true batch dimension specifying\n",
    "            the weight of individual observations.\n",
    "        \"\"\"\n",
    "        if self._shape is None:\n",
    "          # Explicit initialization check.\n",
    "          self._initialize(y_true.shape)\n",
    "        y_true.shape.assert_is_compatible_with(y_pred.shape)\n",
    "        y_true = tf.cast(y_true, 'float32')\n",
    "        y_pred = tf.cast(y_pred, 'float32')\n",
    "\n",
    "        self._product_sum.assign_add(\n",
    "            tf.reduce_sum(y_true * y_pred, axis=self._reduce_axis))\n",
    "\n",
    "        self._true_sum.assign_add(\n",
    "            tf.reduce_sum(y_true, axis=self._reduce_axis))\n",
    "\n",
    "        self._true_squared_sum.assign_add(\n",
    "            tf.reduce_sum(tf.math.square(y_true), axis=self._reduce_axis))\n",
    "\n",
    "        self._pred_sum.assign_add(\n",
    "            tf.reduce_sum(y_pred, axis=self._reduce_axis))\n",
    "\n",
    "        self._pred_squared_sum.assign_add(\n",
    "            tf.reduce_sum(tf.math.square(y_pred), axis=self._reduce_axis))\n",
    "\n",
    "        self._count.assign_add(\n",
    "            tf.reduce_sum(tf.ones_like(y_true), axis=self._reduce_axis))\n",
    "\n",
    "    def result(self):\n",
    "        raise NotImplementedError('Must be implemented in subclasses.')\n",
    "\n",
    "    def reset_states(self):\n",
    "        if self._shape is not None:\n",
    "              tf.keras.backend.batch_set_value([(v, np.zeros(self._shape))\n",
    "                                            for v in self.variables])\n",
    "\n",
    "\n",
    "class PearsonR(CorrelationStats):\n",
    "  \"\"\"Pearson correlation coefficient.\n",
    "\n",
    "  Computed as:\n",
    "  ((x - x_avg) * (y - y_avg) / sqrt(Var[x] * Var[y])\n",
    "  \"\"\"\n",
    "\n",
    "    def __init__(self, reduce_axis=(0,), name='pearsonr'):\n",
    "        \"\"\"Pearson correlation coefficient.\n",
    "\n",
    "        Args:\n",
    "          reduce_axis: Specifies over which axis to compute the correlation.\n",
    "          name: Metric name.\n",
    "        \"\"\"\n",
    "        super(PearsonR, self).__init__(reduce_axis=reduce_axis,\n",
    "                                       name=name)\n",
    "\n",
    "    def result(self):\n",
    "        true_mean = self._true_sum / self._count\n",
    "        pred_mean = self._pred_sum / self._count\n",
    "\n",
    "        covariance = (self._product_sum\n",
    "                      - true_mean * self._pred_sum\n",
    "                      - pred_mean * self._true_sum\n",
    "                      + self._count * true_mean * pred_mean)\n",
    "\n",
    "        true_var = self._true_squared_sum - self._count * tf.math.square(true_mean)\n",
    "        pred_var = self._pred_squared_sum - self._count * tf.math.square(pred_mean)\n",
    "        tp_var = tf.math.sqrt(true_var) * tf.math.sqrt(pred_var)\n",
    "        correlation = covariance / tp_var\n",
    "\n",
    "        return correlation\n",
    "\n",
    "\n",
    "class R2(CorrelationStats):\n",
    "  \"\"\"R-squared  (fraction of explained variance).\"\"\"\n",
    "\n",
    "  def __init__(self, reduce_axis=None, name='R2'):\n",
    "    \"\"\"R-squared metric.\n",
    "\n",
    "    Args:\n",
    "      reduce_axis: Specifies over which axis to compute the correlation.\n",
    "      name: Metric name.\n",
    "    \"\"\"\n",
    "    super(R2, self).__init__(reduce_axis=reduce_axis,\n",
    "                             name=name)\n",
    "\n",
    "    def result(self):\n",
    "        true_mean = self._true_sum / self._count\n",
    "        total = self._true_squared_sum - self._count * tf.math.square(true_mean)\n",
    "        residuals = (self._pred_squared_sum - 2 * self._product_sum\n",
    "                     + self._true_squared_sum)\n",
    "\n",
    "    return tf.ones_like(residuals) - residuals / total\n",
    "\n",
    "\n",
    "class MetricDict:\n",
    "    def __init__(self, metrics):\n",
    "        self._metrics = metrics\n",
    "\n",
    "    def update_state(self, y_true, y_pred):\n",
    "        for k, metric in self._metrics.items():\n",
    "            metric.update_state(y_true, y_pred)\n",
    "    \n",
    "    def result(self):\n",
    "        return {k: metric.result() for k, metric in self._metrics.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataset, head, max_steps=None):\n",
    "    metric = MetricDict({'PearsonR': PearsonR(reduce_axis=(0,1))})\n",
    "    @tf.function\n",
    "    def predict(x):\n",
    "        return model(x, is_training=False)[head]\n",
    "\n",
    "    for i, batch in tqdm(enumerate(dataset)):\n",
    "        if max_steps is not None and i > max_steps:\n",
    "              break\n",
    "        metric.update_state(batch['target'], predict(batch['sequence']))\n",
    "\n",
    "    return metric.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_human = evaluate_model(model,\n",
    "                               dataset=get_dataset('human', 'valid').batch(1).prefetch(2),\n",
    "                               head='human',\n",
    "                               max_steps=100)\n",
    "print('')\n",
    "print({k: v.numpy().mean() for k, v in metrics_human.items()})"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "enformer-training.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
